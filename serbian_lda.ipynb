{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import stanza\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 167392 rows in the minutes dataset.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "file_path = '../data/pre-processed_rs.csv'\n",
    "minutes_rs = pd.read_csv(file_path)\n",
    "print(f'There are {len(minutes_rs)} rows in the minutes dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 97026 rows in the minutes dataset.\n"
     ]
    }
   ],
   "source": [
    "# Remove speeches with word count less than 50\n",
    "minutes_rs = minutes_rs[minutes_rs['speech'].apply(lambda x: len(x.split()) >= 50)]\n",
    "print(f'There are {len(minutes_rs)} rows in the minutes dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedural phrases to remove\n",
    "phrases_to_remove = [\n",
    "    \"Poštovane dame i gospodo narodni poslanici\",\n",
    "    \"Poštovana predsednice Narodne skupštine\",\n",
    "    \"Zahvaljujem gospodi ministar\",\n",
    "    \"Dame i gospodo narodni poslanici,\",\n",
    "    \"Da li još neko od predsednika želi reč?\",\n",
    "    \"Hvala, gospođo predsednice.\",\n",
    "    \"Zahvaljujem.\",\n",
    "    \"Poštovani predsedavajući\",\n",
    "    \"Poštovano predsedništvo, poštovana gospodo ministar\",\n",
    "    \"Koliko imamo vremena, izvinjavam se, koja poslanička grupa?\"\n",
    "                    ]\n",
    "\n",
    "\n",
    "# Create a single regular expression pattern to match all phrases\n",
    "pattern = re.compile(\"|\".join([re.escape(phrase) for phrase in phrases_to_remove]), re.IGNORECASE)\n",
    "\n",
    "# Function to remove specified phrases using regex\n",
    "def remove_phrases(text, pattern):\n",
    "    return pattern.sub(\"\", text)\n",
    "\n",
    "minutes_rs['speech'] = minutes_rs['speech'].apply(lambda x: remove_phrases(str(x), pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Serbian model for Stanza\n",
    "stanza.download('sr')\n",
    "\n",
    "# Initialize the Stanza pipeline for Serbian\n",
    "nlp = stanza.Pipeline('sr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords\n",
    "stopwords_file = '../data/serbian.txt'\n",
    "with open(stopwords_file, 'r', encoding='utf-8') as f:\n",
    "    serbian_stopwords  = set([line.strip() for line in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text, stopwords_list, nlp_pipeline):\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Process text with Stanza\n",
    "    doc = nlp_pipeline(text)\n",
    "    \n",
    "    # Extract lemmas and remove stopwords\n",
    "    tokens = [word.lemma for sentence in doc.sentences for word in sentence.words if word.lemma.lower() not in stopwords_list]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk_file, stopwords_list, nlp_pipeline):\n",
    "    df = pd.read_csv(chunk_file)\n",
    "    df['processed_speech'] = df['speech'].apply(lambda x: preprocess_text(x, stopwords_list, nlp_pipeline))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the tokenization and lemmatization is time-consuming, it's processed in chunks and saved to disk to be combined later\n",
    "\n",
    "#import os\n",
    "#chunk_size = 100\n",
    "#output_dir = '/Users/busraalbayrak/Desktop/chunks'\n",
    "\n",
    "#if not os.path.exists(output_dir):\n",
    "#    os.makedirs(output_dir)\n",
    "\n",
    "#for i, start in enumerate(range(0, len(minutes_rs), chunk_size)):\n",
    "#    end = start + chunk_size\n",
    "#    chunk = minutes_rs[start:end]\n",
    "#    chunk.to_csv(f'{output_dir}/chunk_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_dir = '/Users/busraalbayrak/Desktop/processed'\n",
    "# Process the first 10 chunks\n",
    "# for i in range(10):\n",
    "#     chunk_file = f'{output_dir}/chunk_{i}.csv'\n",
    "#     processed_chunk = process_chunk(chunk_file, serbian_stopwords, nlp)\n",
    "#     processed_chunk.to_csv(f'{processed_dir}/processed_chunk_{i}.csv', index=False)\n",
    "\n",
    "#      ...    \n",
    "\n",
    "# for i in range(960,970):\n",
    "#     chunk_file = f'{output_dir}/chunk_{i}.csv'\n",
    "#     processed_chunk = process_chunk(chunk_file, serbian_stopwords, nlp)\n",
    "#     processed_chunk.to_csv(f'{processed_dir}/processed_chunk_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It's combined and saved as a single file\n",
    "## processed_dir = '../../../Desktop/processed'\n",
    "# combined_df = pd.read_csv(f'{processed_dir}/processed_chunk_0.csv')\n",
    "\n",
    "## Sequentially read and append the remaining chunks\n",
    "# for i in range(1, 970):\n",
    "#     processed_chunk_file = f'{processed_dir}/processed_chunk_{i}.csv'\n",
    "#     processed_chunk = pd.read_csv(processed_chunk_file)\n",
    "#     combined_df = pd.concat([combined_df, processed_chunk], ignore_index=True)\n",
    "\n",
    "# combined_df.to_csv(f'{processed_dir}/minutes_rs.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = '../../../Desktop/processed'\n",
    "minutes_rs = pd.read_csv(f'{processed_dir}/minutes_rs.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and split processed_speech\n",
    "def clean_and_split(speech):\n",
    "    cleaned_speech = speech.replace(\"'\", \" \").replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\").strip()\n",
    "    return cleaned_speech.split()\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "minutes_rs['processed_speech'] = minutes_rs['processed_speech'].apply(clean_and_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check documnet frequency of tokens\n",
    "all_tokens =  [token for sublist in minutes_rs['processed_speech'] for token in sublist]\n",
    "token_counts = Counter(all_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude tokens with common names of MPs and non-informative tokens\n",
    "tokens_to_exclude = ['prof','dr', 'mi', 'on', 'do', 'li', 'jer','svoj', 'po', 'za', 'od', 'ovaj', 'da', 'vi', 'od', 'sa', 'sebe', 'na', 'ne', 'ja', 'moći', 'sav', 'iz',\n",
    "                     'što', 'ili', 'gospodina', 'gospodin', 'ali', 'ako', 'samo', 'morati', 'koliko', 'ko', 'tako', 'zašto', 'zato što', 'pa', 'hteti', 'imati',\n",
    "                     'onda', 'kakav', 'njihov', 'nemati', 'hvala', 'već', 'moj', 'Aleksandra', 'od' 'Dragan', 'Branko', 'Ivan', 'Aleksandar', 'kazati', 'kada', 'dan',\n",
    "                     'Blagoja', 'Marija','Inđija', 'Kovač', 'Pastor','Čanak','Elvir', 'Morava', 'Goran', 'Balint', 'Maja', 'Ivana', 'Tomica', 'Verica', 'ni', 'Srbija',\n",
    "                     'Miloš', 'Miroslav', 'Miroslava', 'Vladan', 'Izvolita' 'Čedomir', 'Bojan', 'Izvolit', 'Jerkov', 'Nataša', 'Jovan', 'Vesna', 'Gordana', 'Šešelj'\n",
    "                    'Marko', 'Dragan', 'Rade', 'Šešelj', 'Izvolita', 'Vladimir', 'Nenad', 'tekst', 'Vjerica', 'deo', 'Šutanovac', 'poljoprivredni', 'dakle', 'videti',\n",
    "                     'Zoran', 'Boško', 'Saša', 'Dušan', 'Marinik', 'Ana', 'Nikola', 'Zdravko', 'Tomislav', 'Radoslav', 'Milan', 'Đorđe', 'Stefan', 'reč', 'Nogo']\n",
    "# Filter tokens with document frequency below 10\n",
    "min_doc_freq = 10\n",
    "tokens_to_keep = {token for token, count in token_counts.items() if count >= min_doc_freq and len(token) > 1 and not (token.endswith('ić') or token in tokens_to_exclude)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['davasti', 'monah', 'mikrofilmovanje']\n"
     ]
    }
   ],
   "source": [
    "#print first 3 tokens to keep\n",
    "print(list(tokens_to_keep)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter tokens in each speech\n",
    "minutes_rs['processed_speech'] = minutes_rs['processed_speech'].apply(lambda tokens: [token for token in tokens if token in tokens_to_keep])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_input = minutes_rs['processed_speech'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/busraalbayrak/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, lowercase=False)\n",
    "X = vectorizer.fit_transform(lda_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_topics = 10\n",
    "n_topics = 20\n",
    "#n_topics = 50\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, \n",
    "                                learning_method='online', \n",
    "                                max_iter=10, \n",
    "                                random_state=0,\n",
    "                                batch_size=1024,\n",
    "                                n_jobs=1 \n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(batch_size=1024, learning_method=&#x27;online&#x27;,\n",
       "                          n_components=20, n_jobs=1, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(batch_size=1024, learning_method=&#x27;online&#x27;,\n",
       "                          n_components=20, n_jobs=1, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(batch_size=1024, learning_method='online',\n",
       "                          n_components=20, n_jobs=1, random_state=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "javan odbor izveštaj medij građanin komisija izbor agencija državni rad pitanje institucija organ skupština izborni savet politički narodni telo član\n",
      "Topic 1:\n",
      "grad opština Beograd godina građanin nov čovek milion velik raditi stan pitanje Vojvodina živeti izgradnja centar evro Niš mesto problem\n",
      "Topic 2:\n",
      "obrazovanje škola kultura godina fakultet student visok nacionalan deca zakon sistem mlad trebati nauka kulturan jezik sport znanje ministar velik\n",
      "Topic 3:\n",
      "zakon izmena predlog javan postupak dopuna odnosno pravni rešenje rok oblast podatak način godina zaštita uslov posao lice nov primena\n",
      "Topic 4:\n",
      "lokalan samouprava budžet sredstvo zakon javan poreski porez uprava prihod republika godina trebati dinar Vojvodina plata građanin budžetski sistem državni\n",
      "Topic 5:\n",
      "sredina životan projekt razvoj saobraćaj infrastruktura godina zaštita izgradnja republika energija velik koridor železnica autoput nov energetski voda projekat pruga\n",
      "Topic 6:\n",
      "narodni skupština poslanik predlog poslovnik zakon član sednica red dnevni poslanički grupa republika stav povreda pretres godina glasanje rad odluka\n",
      "Topic 7:\n",
      "sud sudija postupak ustavan pravo pravosuđe sudski odluka zakon ustav pravni pitanje predmet trebati građanin pravda izvršitelj godina ustava slučaj\n",
      "Topic 8:\n",
      "zakon zdravstven deca rad pravo socijalan zaštita porodica osiguranje žena godina sistem zaposlen radni zdravlje život radnik osoba zapošljavanje poslodavac\n",
      "Topic 9:\n",
      "želeti neko podneti član poslanik zajedno narodni grupa SDS poslanički koron istovetan Balša poslanica mr Dejan Nemanja Marijan hvati Srđan\n",
      "Topic 10:\n",
      "čovek raditi govoriti reći zato trebati misliti građanin neko stranka politički pričati vreme vlast dobro doći kolega malo tu niko\n",
      "Topic 11:\n",
      "krivični lice zakon policija kazna imovina nasilje pravo slučaj mera protiv delo zatvor oružje postupak postojati način zloupotreba takav organ\n",
      "Topic 12:\n",
      "zemlja EU evropski sporazum nacionalan država republika saradnja međunarodni pitanje važan odnos između bezbednost trebati velik oblast manjina pravo strana\n",
      "Topic 13:\n",
      "javan kandidat sud tužilac korupcija tužilaštvo sudija izbor godina visok funkcija protiv borba osnovan kriminal savet sudstvo sudijski pravosudni velik\n",
      "Topic 14:\n",
      "amandman narodni glasanje poslanik član niko podneti prihvatiti stavljati zaključivati konstativati protiv uzdržati saopštavati skupština glasati zajedno odbor grupa poslanički\n",
      "Topic 15:\n",
      "zakon amandman član predlog stav trebati misliti ministar smatrati prihvatiti obrazloženje poštovati govoriti rasprava predložiti vlada razlog značiti pravo postojati\n",
      "Topic 16:\n",
      "godina vlada građanin pitanje govoriti važan zakon ministar misliti velik želeti republika način reći politika zaista ministarstvo nov naravno upravo\n",
      "Topic 17:\n",
      "godina evro milion milijarda banka budžet država dinar velik preduzeće penzija kredit plata dug novac javan zemlja vlada privreda ekonomski\n",
      "Topic 18:\n",
      "srpski republika godina narod država Kosovo Srbin predsednik vojska rat vojni zemlja pitanje KiM poštovati velik vlada odbrana Metohija NATO\n",
      "Topic 19:\n",
      "poljoprivreda godina velik proizvodnja proizvod čovek proizvođač zemlja trebati zdravlje zemljište ministarstvo poljoprivrednik raditi pandemija ministar selo pitanje sredstvo vakcina\n"
     ]
    }
   ],
   "source": [
    "# Display the top words for each topic\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "no_top_words = 20\n",
    "display_topics(lda, vectorizer.get_feature_names_out(), no_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
